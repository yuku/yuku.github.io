(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[382],{911:function(e,s,a){"use strict";a.d(s,{Z:function(){return x}});a(7294);var n=a(9008),t=a(1664),l=a(1163),c=a(4184),i=a.n(c),r=a(381),o=a.n(r),d=a(7814),h=a(4007),p=a(5893),m=function(e){return(0,p.jsxs)("aside",{className:i()("widget mb-4",e.className),children:[(0,p.jsx)("h1",{className:"title font-weight-bold",children:e.title}),e.children]})},u=function(e){return e||"https://yuku.takahashi.coffee".concat(h.hs)},x=(0,l.withRouter)((function(e){return(0,p.jsxs)("div",{className:"blogpage container",children:[(0,p.jsxs)(n.default,{children:[(0,p.jsxs)("title",{children:[e.meta.title," - ",h.px]}),(0,p.jsx)("meta",{name:"description",content:e.meta.description}),(0,p.jsx)("meta",{name:"twitter:card",content:"summary"}),(0,p.jsx)("meta",{name:"twitter:creator",content:"@yuku_t"}),(0,p.jsx)("meta",{property:"fb:app_id",content:h.f6}),(0,p.jsx)("meta",{property:"og:title",content:"".concat(e.meta.title," - ").concat(h.px)}),(0,p.jsx)("meta",{property:"og:type",content:"article"}),(0,p.jsx)("meta",{property:"og:url",content:"https://yuku.takahashi.coffee".concat(e.router.pathname)}),(0,p.jsx)("meta",{property:"og:image",content:u()}),(0,p.jsx)("meta",{property:"og:description",content:e.meta.description}),(0,p.jsx)("link",{rel:"stylesheet",href:"https://cdnjs.cloudflare.com/ajax/libs/prism/1.15.0/themes/prism.min.css"}),(0,p.jsx)("link",{rel:"stylesheet",href:"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css"})]}),(0,p.jsxs)("div",{className:"row",children:[(0,p.jsx)("main",{className:i()("col-xl-8",e.className),children:(0,p.jsxs)("article",{itemScope:!0,itemType:"http://schema.org/BlogPosting",children:[(0,p.jsx)("meta",{itemProp:"author",content:"Yuku Takahashi"}),(0,p.jsx)("meta",{itemProp:"datePublished",content:e.meta.publishedAt}),e.meta.modifiedAt&&(0,p.jsx)("meta",{itemProp:"dateModified",content:e.meta.modifiedAt}),(0,p.jsx)("meta",{itemProp:"image",content:u()}),(0,p.jsxs)("header",{className:"mb-4 header",children:[(0,p.jsx)("h1",{className:"headline",itemProp:"headline",children:e.meta.title}),(0,p.jsxs)("ul",{className:"list-inline text-dark font-weight-light",children:[(0,p.jsx)("li",{className:"list-inline-item",children:(0,p.jsx)("span",{className:"ml-1",children:o()(e.meta.publishedAt).format("YYYY-MM-DD HH:mm")})}),(0,p.jsx)("li",{className:"list-inline-item",children:e.meta.tags.map((function(e,s){return[s>0?",":null,(0,p.jsxs)("span",{className:"ml-1",children:["#",e]},s)]}))})]})]}),(0,p.jsx)("section",{className:"mb-4 body",itemProp:"articleBody",children:e.children})]})}),(0,p.jsx)("div",{className:"col-xl-4",children:(0,p.jsxs)("div",{className:"pl-xl-2",children:[(0,p.jsx)(m,{title:"About Me",children:(0,p.jsxs)("div",{children:[(0,p.jsx)("img",{src:h.hs,alt:"avatar",className:"avatar rounded-circle mb-4 mx-auto d-block"}),(0,p.jsxs)("p",{children:["Software Engineer at FLYWHEEL. Working mainly on recommendation systems in these days. Ex-Qiita CTO.",(0,p.jsx)(t.default,{href:"/about",children:(0,p.jsx)("a",{href:"/about",style:{marginLeft:"4px"},children:"Read more"})}),"."]})]})}),(0,p.jsx)(m,{title:"Follow",children:(0,p.jsxs)("ul",{className:"list-inline icons",children:[(0,p.jsx)("li",{className:"list-inline-item",children:(0,p.jsx)("a",{href:"https://twitter.com/yuku_t",children:(0,p.jsxs)("span",{className:"fa-stack fa-lg",children:[(0,p.jsx)(d.G,{icon:"circle",className:"fa-stack-2x"}),(0,p.jsx)(d.G,{icon:["fab","twitter"],inverse:!0,className:"fa-stack-1x"})]})})}),(0,p.jsx)("li",{className:"list-inline-item",children:(0,p.jsx)("a",{href:"https://github.com/yuku",children:(0,p.jsxs)("span",{className:"fa-stack fa-lg",children:[(0,p.jsx)(d.G,{icon:"circle",className:"fa-stack-2x"}),(0,p.jsx)(d.G,{icon:["fab","github"],inverse:!0,className:"fa-stack-1x"})]})})}),(0,p.jsx)("li",{className:"list-inline-item",children:(0,p.jsx)("a",{href:"/static/rss-feed.xml",children:(0,p.jsxs)("span",{className:"fa-stack fa-lg",children:[(0,p.jsx)(d.G,{icon:"circle",className:"fa-stack-2x"}),(0,p.jsx)(d.G,{icon:"rss",inverse:!0,className:"fa-stack-1x"})]})})})]})})]})})]})]})}))},1539:function(e,s,a){"use strict";a(7294);var n=a(911),t=a(5893);s.Z=function(e){return(0,t.jsx)(n.Z,{className:"notebook",meta:e.meta,children:e.children})}},476:function(e,s,a){"use strict";a.r(s),a.d(s,{metadata:function(){return l}});a(7294);var n=a(1539),t=a(5893),l={description:"An introduction to pyspark using the Docker image environment provided by Jupyter Lab.",modifiedAt:"2020-05-30T08:34:49+09:00",publishedAt:"2019-01-16T20:50:00+09:00",tags:["pyspark"],title:"Getting Started With Pyspark Using Docker"};s.default=function(){return(0,t.jsx)(n.Z,{meta:l,children:(0,t.jsx)("div",{className:"nb-notebook",children:(0,t.jsxs)("div",{className:"nb-worksheet",children:[(0,t.jsxs)("div",{className:"nb-cell nb-markdown-cell",children:[(0,t.jsxs)("p",{children:[(0,t.jsx)("a",{href:"https://spark.apache.org",children:"Apatch Spark"})," is an open source distributed programming environment implemented on top of the JVM that has seen ",(0,t.jsx)("a",{href:"http://fortune.com/2015/09/25/apache-spark-survey/",children:"a rapid rise in popularity in recent years"}),". I will use Spark through ",(0,t.jsx)("a",{href:"https://spark.apache.org/docs/latest/api/python/index.html",children:"pyspark"})," at my next job, so I want to use pyspark, but it's hard to install Spark from scratch because I'm not familiar with the JVM."]}),(0,t.jsx)("p",{children:"So in this article, I'll use Docker to build Spark and pyspark environments."}),(0,t.jsx)("h2",{children:"Set Up an Environment"}),(0,t.jsxs)("p",{children:["There is a docker image named ",(0,t.jsx)("a",{href:"https://hub.docker.com/r/jupyter/pyspark-notebook/",children:"jupyter/pyspark-notebook"})," published by Jupyter Lab. For now, let's pull the latest version:"]}),(0,t.jsx)("pre",{className:"language-bash",children:(0,t.jsxs)("code",{className:"language-bash",children:["docker pull jupyter/pyspark-notebook:87210526f381","\n"]})}),(0,t.jsx)("p",{children:"Run it:"}),(0,t.jsx)("pre",{className:"language-bash",children:(0,t.jsxs)("code",{className:"language-bash",children:["docker run --rm -w /app -p ",(0,t.jsx)("span",{className:"token number",children:"8888"}),":8888 ",(0,t.jsx)("span",{className:"token punctuation",children:"\\"}),"\n","    ","--mount ",(0,t.jsx)("span",{className:"token assign-left variable",children:"type"}),(0,t.jsx)("span",{className:"token operator",children:"="}),"bind,src",(0,t.jsx)("span",{className:"token operator",children:"="}),(0,t.jsxs)("span",{className:"token variable",children:[(0,t.jsx)("span",{className:"token variable",children:"$("}),(0,t.jsx)("span",{className:"token builtin class-name",children:"pwd"}),(0,t.jsx)("span",{className:"token variable",children:")"})]}),",dst",(0,t.jsx)("span",{className:"token operator",children:"="}),"/app ",(0,t.jsx)("span",{className:"token punctuation",children:"\\"}),"\n","    ","jupyter/pyspark-notebook:87210526f381","\n"]})}),(0,t.jsx)("p",{children:"Then you will see several messages, among which is the URL. If you access that URL, you will see a Jupyter Notebook that you can use with pyspark:"})]}),(0,t.jsxs)("div",{className:"nb-cell nb-code-cell",children:[(0,t.jsx)("div",{className:"nb-input","data-prompt-number":1,children:(0,t.jsx)("pre",{className:"language-python",children:(0,t.jsxs)("code",{className:"language-python","data-language":"python",children:[(0,t.jsx)("span",{className:"token keyword",children:"import"})," pyspark","\n","pyspark",(0,t.jsx)("span",{className:"token punctuation",children:"."}),"version",(0,t.jsx)("span",{className:"token punctuation",children:"."}),"__version__"]})})}),(0,t.jsx)("div",{className:"nb-output","data-prompt-number":1,children:(0,t.jsx)("pre",{className:"nb-text-output",children:"'2.4.0'"})})]}),(0,t.jsxs)("div",{className:"nb-cell nb-markdown-cell",children:[(0,t.jsx)("p",{children:"Note that this article is written using Jupyter Notebook, which was launched exactly in this way."}),(0,t.jsx)("h2",{children:"Launching a Spark Cluster"}),(0,t.jsxs)("p",{children:["Spark usually creates a cluster in distributed environment, but creating a distributed cluster in development is not a big deal, so there is ",(0,t.jsx)("a",{href:"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html",children:"a local mode"}),"."]}),(0,t.jsxs)("p",{children:["To start Spark in local mode via pyspark, call ",(0,t.jsx)("a",{href:"https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext",children:(0,t.jsx)("code",{children:"pyspark.SparkContext"})}),":"]})]}),(0,t.jsx)("div",{className:"nb-cell nb-code-cell",children:(0,t.jsx)("div",{className:"nb-input","data-prompt-number":2,children:(0,t.jsx)("pre",{className:"language-python",children:(0,t.jsxs)("code",{className:"language-python","data-language":"python",children:["sc ",(0,t.jsx)("span",{className:"token operator",children:"="})," pyspark",(0,t.jsx)("span",{className:"token punctuation",children:"."}),"SparkContext",(0,t.jsx)("span",{className:"token punctuation",children:"("}),(0,t.jsx)("span",{className:"token string",children:"'local[*]'"}),(0,t.jsx)("span",{className:"token punctuation",children:")"})]})})})}),(0,t.jsxs)("div",{className:"nb-cell nb-markdown-cell",children:[(0,t.jsx)("p",{children:"The string specifies the number available threads:"}),(0,t.jsxs)("ul",{children:[(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"local"})," - 1 thread"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"local[n]"})," - ",(0,t.jsx)("code",{children:"n"})," threads\uff08",(0,t.jsx)("code",{children:"n"})," is a number\uff09"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"local[*]"})," - As many threads as available in JVM.\uff08",(0,t.jsx)("a",{href:"https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--",children:(0,t.jsx)("code",{children:"Runtime.getRuntime.availableProcessors()"})})," is used internally\uff09"]})]}),(0,t.jsxs)("p",{children:["It seems that ",(0,t.jsx)("code",{children:"local[*]"})," is commonly used."]}),(0,t.jsx)("p",{children:"Try to calculate the sum of the numbers from 0 to 10."})]}),(0,t.jsxs)("div",{className:"nb-cell nb-code-cell",children:[(0,t.jsx)("div",{className:"nb-input","data-prompt-number":3,children:(0,t.jsx)("pre",{className:"language-python",children:(0,t.jsxs)("code",{className:"language-python","data-language":"python",children:["rdd ",(0,t.jsx)("span",{className:"token operator",children:"="})," sc",(0,t.jsx)("span",{className:"token punctuation",children:"."}),"parallelize",(0,t.jsx)("span",{className:"token punctuation",children:"("}),(0,t.jsx)("span",{className:"token builtin",children:"range"}),(0,t.jsx)("span",{className:"token punctuation",children:"("}),(0,t.jsx)("span",{className:"token number",children:"10"}),(0,t.jsx)("span",{className:"token punctuation",children:")"}),(0,t.jsx)("span",{className:"token punctuation",children:")"}),"\n","rdd",(0,t.jsx)("span",{className:"token punctuation",children:"."}),(0,t.jsx)("span",{className:"token builtin",children:"sum"}),(0,t.jsx)("span",{className:"token punctuation",children:"("}),(0,t.jsx)("span",{className:"token punctuation",children:")"})]})})}),(0,t.jsx)("div",{className:"nb-output","data-prompt-number":3,children:(0,t.jsx)("pre",{className:"nb-text-output",children:"45"})})]}),(0,t.jsx)("div",{className:"nb-cell nb-markdown-cell",children:(0,t.jsx)("p",{children:"Stop the cluster when you are done using it."})}),(0,t.jsx)("div",{className:"nb-cell nb-code-cell",children:(0,t.jsx)("div",{className:"nb-input","data-prompt-number":4,children:(0,t.jsx)("pre",{className:"language-python",children:(0,t.jsxs)("code",{className:"language-python","data-language":"python",children:["sc",(0,t.jsx)("span",{className:"token punctuation",children:"."}),"stop",(0,t.jsx)("span",{className:"token punctuation",children:"("}),(0,t.jsx)("span",{className:"token punctuation",children:")"})]})})})}),(0,t.jsxs)("div",{className:"nb-cell nb-markdown-cell",children:[(0,t.jsx)("h2",{children:"Conclusion"}),(0,t.jsx)("p",{children:"In this article, I created a pyspark environment using Docker and launched a Spark cluster in a local mode. I don't know much about Spark yet, but I'll try to do more and more things little by little."}),(0,t.jsx)("h2",{children:"References"}),(0,t.jsxs)("ul",{children:[(0,t.jsx)("li",{children:(0,t.jsx)("a",{href:"https://hub.docker.com/r/jupyter/pyspark-notebook/",children:"jupyter/pyspark-notebook - Docker Hub"})}),(0,t.jsx)("li",{children:(0,t.jsx)("a",{href:"https://jupyter-docker-stacks.readthedocs.io/en/latest/using/specifics.html#apache-spark",children:"Image Specifics \u2014 docker-stacks latest documentation"})}),(0,t.jsx)("li",{children:(0,t.jsx)("a",{href:"https://spark.apache.org/docs/latest/api/python/pyspark.html",children:"pyspark package \u2014 PySpark 2.4.0 documentation"})}),(0,t.jsx)("li",{children:(0,t.jsx)("a",{href:"https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f",children:"Get Started with PySpark and Jupyter Notebook in 3 Minutes"})}),(0,t.jsx)("li",{children:(0,t.jsx)("a",{href:"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html",children:"Spark local (pseudo-cluster) \xb7 Mastering Apache Spark"})})]})]})]})})})}},4485:function(e,s,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/blog/2019/01/hello-pyspark",function(){return a(476)}])}},function(e){e.O(0,[885,774,888,179],(function(){return s=4485,e(e.s=s);var s}));var s=e.O();_N_E=s}]);