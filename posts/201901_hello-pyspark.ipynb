{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apatch Spark](https://spark.apache.org) is an open source distributed programming environment implemented on top of the JVM that has seen [a rapid rise in popularity in recent years](http://fortune.com/2015/09/25/apache-spark-survey/). I will use Spark through [pyspark](https://spark.apache.org/docs/latest/api/python/index.html) at my next job, so I want to use pyspark, but it's hard to install Spark from scratch because I'm not familiar with the JVM.\n",
    "\n",
    "So in this article, I'll use Docker to build Spark and pyspark environments.\n",
    "\n",
    "## Set Up an Environment\n",
    "\n",
    "There is a docker image named [jupyter/pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook/) published by Jupyter Lab. For now, let's pull the latest version:\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook:87210526f381\n",
    "```\n",
    "\n",
    "Run it:\n",
    "\n",
    "```bash\n",
    "docker run --rm -w /app -p 8888:8888 \\\n",
    "    --mount type=bind,src=$(pwd),dst=/app \\\n",
    "    jupyter/pyspark-notebook:87210526f381\n",
    "```\n",
    "\n",
    "Then you will see several messages, among which is the URL. If you access that URL, you will see a Jupyter Notebook that you can use with pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this article is written using Jupyter Notebook, which was launched exactly in this way.\n",
    "\n",
    "## Launching a Spark Cluster\n",
    "\n",
    "Spark usually creates a cluster in distributed environment, but creating a distributed cluster in development is not a big deal, so there is [a local mode](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html).\n",
    "\n",
    "To start Spark in local mode via pyspark, call [`pyspark.SparkContext`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string specifies the number available threads:\n",
    "\n",
    "- `local` - 1 thread\n",
    "- `local[n]` - `n` threads（`n` is a number）\n",
    "- `local[*]` - As many threads as available in JVM.（[`Runtime.getRuntime.availableProcessors()`](https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#availableProcessors--) is used internally）\n",
    "\n",
    "It seems that `local[*]` is commonly used.\n",
    "\n",
    "Try to calculate the sum of the numbers from 0 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "rdd.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the cluster when you are done using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this article, I created a pyspark environment using Docker and launched a Spark cluster in a local mode. I don't know much about Spark yet, but I'll try to do more and more things little by little.\n",
    "\n",
    "## References\n",
    "\n",
    "- [jupyter/pyspark-notebook - Docker Hub](https://hub.docker.com/r/jupyter/pyspark-notebook/)\n",
    "- [Image Specifics — docker-stacks latest documentation](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/specifics.html#apache-spark)\n",
    "- [pyspark package — PySpark 2.4.0 documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "- [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)\n",
    "- [Spark local (pseudo-cluster) · Mastering Apache Spark](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-local.html)"
   ]
  }
 ],
 "metadata": {
  "post_metadata": {
   "description": "An introduction to pyspark using the Docker image environment provided by Jupyter Lab.",
   "modifiedAt": "2020-05-30T08:34:49+09:00",
   "publishedAt": "2019-01-16T20:50:00+09:00",
   "tags": ["pyspark"],
   "title": "Getting Started With Pyspark Using Docker"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
